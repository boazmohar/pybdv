{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybdv\n",
    "import os\n",
    "try:\n",
    "    import dask.array as da\n",
    "    import zarr\n",
    "    has_dask = True\n",
    "except ImportError:\n",
    "    has_dask = False\n",
    "import numpy as np\n",
    "from importlib import reload  \n",
    "reload(pybdv) \n",
    "import pybdv\n",
    "from pybdv.converter import make_bdv_from_dask_array, \\\n",
    "normalize_output_path_dask, handle_setup_id, validate_attributes\n",
    "from pybdv.util import get_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = da.random.randint(0,1000,(10,20,30))\n",
    "np_array = data.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'E:\\\\test.zarr'\n",
    "setup_id=None\n",
    "timepoint=0\n",
    "setup_name=None\n",
    "affine=None\n",
    "attributes={'channel': {'id': None}}\n",
    "overwrite='all'\n",
    "chunks=None\n",
    "downscale_factors=[[2,2,2], [2,2,2]]\n",
    "downsample_chunks = ((5,5,5), (1,1,1))\n",
    "downscale_func=np.mean\n",
    "resolution=[1., 1., 1.]\n",
    "unit='pixel'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not has_dask:\n",
    "    raise ImportError(\"Please install dask to use this function\")\n",
    "if not isinstance(data, da.Array):\n",
    "    raise ValueError(\"Input needs to be dask array, got %s\" % type(data))\n",
    "ndim = data.ndim\n",
    "if ndim != 3 or len(resolution) != ndim:\n",
    "    raise ValueError(\"Invalid input dimensionality\")\n",
    "if affine is not None:\n",
    "    validate_affine(affine)\n",
    "is_h5 = False\n",
    "data_path, xml_path, is_n5 = normalize_output_path_dask(output_path)\n",
    "setup_id, overwrite_data, overwrite_metadata, skip = handle_setup_id(setup_id,\n",
    "                                                                        xml_path,\n",
    "                                                                        timepoint,\n",
    "                                                                        overwrite,\n",
    "                                                                        is_h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'channel': {'id': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enforce_consistency = not (overwrite_data or overwrite_metadata)\n",
    "attributes_ = validate_attributes(xml_path, attributes, setup_id, enforce_consistency)\n",
    "attributes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if chunks is not None:\n",
    "        data = data.rechunk(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " if downscale_factors is None:\n",
    "        # set single level downscale factor\n",
    "        factors = [[1, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scales_dask(data, data_path, is_n5, downscale_factors, downscale_func,\n",
    "                ndim, setup_id, downsample_chunks=None, timepoint=0, overwrite=False):\n",
    "    if not all(isinstance(factor, (int, tuple, list)) for factor in downscale_factors):\n",
    "        raise ValueError(\"Invalid downscale factor\")\n",
    "    if not all(len(factor) == 3 for factor in downscale_factors\n",
    "               if isinstance(factor, (tuple, list))):\n",
    "        raise ValueError(\"Invalid downscale factor\")\n",
    "    # normalize all factors to be tuple or list\n",
    "    factors = [ndim*[factor] if isinstance(factor, int) else factor\n",
    "               for factor in downscale_factors]\n",
    "    # make sure downsample chunks are also 3 items for each downsample factor\n",
    "    if downsample_chunks is not None:\n",
    "        if not all(len(chunks) == 3 for chunks in downsample_chunks):\n",
    "            raise ValueError(\"Invalid downscale chunks\")\n",
    "    else:\n",
    "        downsample_chunks = tuple((64,64,64) for _ in range(len(downscale_factors)))\n",
    "    # run single downsampling stages\n",
    "\n",
    "    pyramid = {}\n",
    "    pyramid['s0'] = data\n",
    "    current_factor = np.array([1,1,1])\n",
    "    for scale, (factor, chunks) in enumerate(zip(factors, downsample_chunks)):\n",
    "        key_ = 's%d' % (scale + 1)\n",
    "        current_factor *= factor\n",
    "        factor_dict = {k: v for k, v in zip(range(ndim), current_factor)}\n",
    "        print(f'key: {key_}, factor: {current_factor}, dict: {factor_dict}, chunks:{chunks}')\n",
    "        pyramid[key_] = da.coarsen(downscale_func, data, factor_dict, trim_excess=True).rechunk(chunks)\n",
    "    base_key = get_key(is_h5=False, timepoint=timepoint, setup_id=setup_id, scale=0)\n",
    "    path_all = os.path.join(data_path, base_key[:-2])\n",
    "    if is_n5:\n",
    "        store = zarr.N5FSStore(path_all)\n",
    "    else:\n",
    "        store = zarr.DirectoryStore(path_all)\n",
    "    group = zarr.open(store, mode='w')\n",
    "    save_chunks_all = ((data.chunksize),) + downsample_chunks\n",
    "    arrays = []\n",
    "    for (k,v), save_chunks in zip(pyramid.items(), save_chunks_all):\n",
    "        arrays.append(group.zeros(name=k, shape=v.shape, dtype=v.dtype, \n",
    "        chunks=save_chunks, compressor=GZip()))\n",
    "    da.store(pyramid.values(), arrays, lock=None)\n",
    "    # add first level to factors\n",
    "    factors = [[1, 1, 1]] + factors\n",
    "    return factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20, 30)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: s1, factor: [2 2 2], dict: {0: 2, 1: 2, 2: 2}, chunks:(64, 64, 64)\n",
      "key: s2, factor: [4 4 4], dict: {0: 4, 1: 4, 2: 4}, chunks:(64, 64, 64)\n",
      "setup0/timepoint0/s0\n"
     ]
    }
   ],
   "source": [
    "factors= make_scales_dask(data, data_path, is_n5, downscale_factors, downscale_func,\n",
    "                    ndim, setup_id, downsample_chunks=None,\n",
    "                    timepoint=timepoint, overwrite=overwrite_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numcodecs import GZip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 5, 5), (1, 1, 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(downsample_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1], [2, 2, 2], [2, 2, 2]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_disk = zarr.open_array(r'E:\\test.zarr\\setup0\\timepoint0\\s0')\n",
    "np.allclose(np_array, from_disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyramid = {}\n",
    "pyramid['s0'] = upscaled_45.astype(np.uint16).rechunk((128, 128,128))\n",
    "pyramid['s1'] = da.coarsen(reducer, upscaled_45, {k: 2 for k in range(upscaled_45.ndim)}, \n",
    "                            trim_excess=True).astype(np.uint16).rechunk((128, 128,128))\n",
    "pyramid['s2'] = da.coarsen(reducer, upscaled_45, {k: 4 for k in range(upscaled_45.ndim)}, \n",
    "                            trim_excess=True).astype(np.uint16).rechunk((128, 128,128))\n",
    "pyramid['s3'] = da.coarsen(reducer, upscaled_45, {k: 8 for k in range(upscaled_45.ndim)}, \n",
    "                            trim_excess=True).astype(np.uint16).rechunk((64, 64,64))\n",
    "pyramid['s4'] = da.coarsen(reducer, upscaled_45, {k: 16 for k in range(upscaled_45.ndim)}, \n",
    "                            trim_excess=True).astype(np.uint16).rechunk((64, 64,64))\n",
    "pyramid['s5'] = da.coarsen(reducer, pyramid['s4'], {k: 2 for k in range(upscaled_45.ndim)}, \n",
    "                            trim_excess=True).astype(np.uint16).rechunk((64, 64,64))\n",
    "n5_path = filename + f'/tile_{pos:03}_ch{view}'\n",
    "group = zarr.open(zarr.N5Store(n5_path), mode='w')\n",
    "group.attrs.update(neuroglancer_attributes)\n",
    "save_chunks_all = [(128, 128,128), (128, 128,128), (128, 128,128), (64, 64,64), (64, 64,64), (64, 64,64)]\n",
    "arrays = []\n",
    "for (k,v), save_chunks in zip(pyramid.items(), save_chunks_all):\n",
    "    arrays.append(group.zeros(name=k, shape=v.shape, dtype=v.dtype, chunks=save_chunks, compressor='gzip'))\n",
    "da.store(pyramid.values(), arrays, lock=None)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "500bfe58a864d56755e08dfcef1ff6b1782e46b764fa0ffdc51eba4d23a066ce"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pybdv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
